model: "lstm"
base_config: "configs/base.yaml"
factors_config: "configs/factors.yaml"

trainer:
  batch_size: 256
  epochs: 50
  early_stop_patience: 10
  lr: 0.0005
  weight_decay: 0.0001
  lr_scheduler: "plateau"
  grad_clip: 1.0

lstm:
  hidden_size: 128
  num_layers: 3
  dropout: 0.3
  bidirectional: true

loss:
  bce_weight: 1.0
  pinball_weight: 1.0
  use_focal: true
  focal_alpha: 0.75
  focal_gamma: 2.5

output_dir: "runs/lstm"

